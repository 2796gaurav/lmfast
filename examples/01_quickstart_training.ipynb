{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üöÄ LMFast Quickstart: Train Your First SLM\n",
                "\n",
                "**Train a production-ready Small Language Model in 5 minutes on free Colab T4!**\n",
                "\n",
                "## What You'll Learn\n",
                "- How to install LMFast\n",
                "- Train a SmolLM-135M model with custom data\n",
                "- Save and export your model\n",
                "- Generate text with your fine-tuned model\n",
                "\n",
                "## Prerequisites\n",
                "- Google Colab with T4 GPU (free tier works!)\n",
                "- Basic Python knowledge\n",
                "\n",
                "**Time to complete:** ~10 minutes (including training)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1Ô∏è‚É£ Setup - One-Click Install"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install LMFast with all dependencies\n",
                "!pip install -q lmfast[all]\n",
                "\n",
                "# Verify GPU is available\n",
                "import torch\n",
                "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
                "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import LMFast\n",
                "import lmfast\n",
                "\n",
                "# Setup Colab environment (optimizes for T4)\n",
                "lmfast.setup_colab_env()\n",
                "\n",
                "print(f\"LMFast version: {lmfast.__version__}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2Ô∏è‚É£ Prepare Your Data\n",
                "\n",
                "LMFast accepts data in multiple formats:\n",
                "- HuggingFace datasets\n",
                "- JSON files\n",
                "- CSV files\n",
                "- Python lists\n",
                "\n",
                "For this quickstart, we'll use the Alpaca dataset (instruction-following)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from datasets import load_dataset\n",
                "\n",
                "# Load a small subset of Alpaca for quick training\n",
                "dataset = load_dataset(\"yahma/alpaca-cleaned\", split=\"train[:1000]\")\n",
                "\n",
                "print(f\"Dataset size: {len(dataset)} examples\")\n",
                "print(f\"\\nExample entry:\")\n",
                "print(dataset[0])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3Ô∏è‚É£ Configure Your Model\n",
                "\n",
                "LMFast uses Pydantic configs for validation. All defaults are optimized for Colab T4."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from lmfast import SLMConfig, TrainingConfig\n",
                "\n",
                "# Model configuration\n",
                "model_config = SLMConfig(\n",
                "    model_name=\"HuggingFaceTB/SmolLM-135M\",  # Fastest model for learning\n",
                "    max_seq_length=512,\n",
                "    load_in_4bit=True,  # QLoRA for memory efficiency\n",
                ")\n",
                "\n",
                "# Training configuration\n",
                "training_config = TrainingConfig(\n",
                "    output_dir=\"./my_first_slm\",\n",
                "    max_steps=200,  # ~5 min on T4\n",
                "    batch_size=4,\n",
                "    gradient_accumulation_steps=4,\n",
                "    learning_rate=2e-4,\n",
                "    lora_r=16,\n",
                "    lora_alpha=32,\n",
                ")\n",
                "\n",
                "# Check T4 compatibility\n",
                "print(f\"T4 Compatible: {model_config.is_colab_t4_compatible()}\")\n",
                "print(f\"Effective Batch Size: {training_config.effective_batch_size}\")\n",
                "print(f\"Estimated Training Time: {training_config.estimated_training_time_minutes:.0f} min\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4Ô∏è‚É£ Train Your Model! üéØ\n",
                "\n",
                "This is where the magic happens. LMFast handles:\n",
                "- Data formatting and tokenization\n",
                "- LoRA adapter injection\n",
                "- GPU memory optimization\n",
                "- Progress tracking"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from lmfast import SLMTrainer\n",
                "\n",
                "# Create trainer\n",
                "trainer = SLMTrainer(\n",
                "    model_config=model_config,\n",
                "    training_config=training_config,\n",
                "    use_unsloth=True  # 2-5x faster training!\n",
                ")\n",
                "\n",
                "# Train!\n",
                "print(\"üöÄ Starting training...\")\n",
                "trainer.train(dataset)\n",
                "print(\"‚úÖ Training complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5Ô∏è‚É£ Save Your Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save the trained model\n",
                "trainer.save(\"./my_first_slm\")\n",
                "print(\"‚úÖ Model saved to ./my_first_slm\")\n",
                "\n",
                "# List saved files\n",
                "import os\n",
                "for f in os.listdir(\"./my_first_slm\"):\n",
                "    size_mb = os.path.getsize(f\"./my_first_slm/{f}\") / 1e6\n",
                "    print(f\"  {f}: {size_mb:.1f} MB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6Ô∏è‚É£ Test Your Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from lmfast.inference import SLMServer\n",
                "\n",
                "# Create inference server\n",
                "server = SLMServer(\"./my_first_slm\")\n",
                "\n",
                "# Test generation\n",
                "prompts = [\n",
                "    \"What is machine learning?\",\n",
                "    \"Write a haiku about programming.\",\n",
                "    \"Explain the benefits of small language models.\"\n",
                "]\n",
                "\n",
                "for prompt in prompts:\n",
                "    print(f\"\\nüìù Prompt: {prompt}\")\n",
                "    response = server.generate(\n",
                "        prompt,\n",
                "        max_new_tokens=100,\n",
                "        temperature=0.7\n",
                "    )\n",
                "    print(f\"ü§ñ Response: {response}\")\n",
                "    print(\"-\" * 50)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7Ô∏è‚É£ (Optional) Export to GGUF for Local Use\n",
                "\n",
                "Export your model for use with llama.cpp, Ollama, or other local inference engines."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from lmfast.inference.quantization import export_gguf\n",
                "\n",
                "# Export to GGUF (requires llama.cpp - may not work in Colab)\n",
                "try:\n",
                "    export_gguf(\n",
                "        \"./my_first_slm\",\n",
                "        \"./my_first_slm.gguf\",\n",
                "        quantization=\"q4_k_m\"\n",
                "    )\n",
                "    print(\"‚úÖ GGUF export complete!\")\n",
                "except Exception as e:\n",
                "    print(f\"‚ö†Ô∏è GGUF export requires llama.cpp: {e}\")\n",
                "    print(\"üí° Tip: For GGUF export, use a local machine with llama.cpp installed.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üéâ Congratulations!\n",
                "\n",
                "You've just trained your first Small Language Model with LMFast!\n",
                "\n",
                "### Next Steps\n",
                "\n",
                "1. **Custom Data**: See `02_custom_dataset.ipynb` for data preparation\n",
                "2. **Better Performance**: See `05_knowledge_distillation.ipynb` for distillation\n",
                "3. **Build Agents**: See `09_basic_agents.ipynb` for tool-using agents\n",
                "4. **Production Deploy**: See `04_inference_server.ipynb` for serving\n",
                "\n",
                "### Key Takeaways\n",
                "\n",
                "- ‚úÖ Train SLMs on free Colab T4 in minutes\n",
                "- ‚úÖ LMFast handles optimization automatically\n",
                "- ‚úÖ Export to multiple formats (GGUF, INT4, etc.)\n",
                "- ‚úÖ Simple API: just 5 lines of code to train!\n",
                "\n",
                "### Resources\n",
                "\n",
                "- üìö [Documentation](https://lmfast.readthedocs.io)\n",
                "- üí¨ [Discord Community](https://discord.gg/lmfast)\n",
                "- üêõ [GitHub Issues](https://github.com/2796gaurav/LMFast/issues)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# One-line training alternative!\n",
                "# If you want the simplest possible experience:\n",
                "\n",
                "# import lmfast\n",
                "# lmfast.train(\n",
                "#     model=\"HuggingFaceTB/SmolLM-135M\",\n",
                "#     dataset=\"yahma/alpaca-cleaned\",\n",
                "#     output_dir=\"./quick_model\",\n",
                "#     max_steps=200\n",
                "# )"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
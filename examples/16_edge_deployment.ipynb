{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üì± LMFast: Edge Deployment\n",
                "\n",
                "**Run SLMs on Raspberry Pi, Android, and Edge Devices!**\n",
                "\n",
                "## What You'll Learn\n",
                "- Export models to GGUF format\n",
                "- Run models with `llama.cpp`\n",
                "- Optimize for low-RAM devices (RPi 4/5)\n",
                "- Build a simple terminal chat app for edge\n",
                "\n",
                "## Supported Hardware\n",
                "- **Raspberry Pi 4/5** (4GB+ RAM)\n",
                "- **Android Phones** (via UserLAnd or Termux)\n",
                "- **NVIDIA Jetson Nano**\n",
                "- **Laptops** (Mac M1/M2/M3, Windows, Linux)\n",
                "\n",
                "**Time to complete:** ~15 minutes"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1Ô∏è‚É£ Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install standard tools\n",
                "!pip install -q lmfast[all]\n",
                "\n",
                "# Install python bindings for llama.cpp\n",
                "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python  # Use CUBLAS if on Colab GPU\n",
                "# On Raspberry Pi: pip install llama-cpp-python (no CMAKE_ARGS needed usually)\n",
                "\n",
                "import lmfast\n",
                "lmfast.setup_colab_env()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2Ô∏è‚É£ Export to GGUF\n",
                "\n",
                "We need the GGUF format for efficient CPU/Edge inference."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from lmfast.inference import export_gguf\n",
                "\n",
                "model_id = \"HuggingFaceTB/SmolLM-135M-Instruct\"\n",
                "\n",
                "# Export to 4-bit quantized GGUF\n",
                "# This reduces size from ~270MB to ~100MB\n",
                "export_gguf(\n",
                "    model_path=model_id,\n",
                "    output_path=\"./smollm-135m-q4.gguf\",\n",
                "    quantization=\"q4_k_m\"\n",
                ")\n",
                "\n",
                "print(\"‚úÖ Exported: ./smollm-135m-q4.gguf\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3Ô∏è‚É£ Run on \"Edge\" (Simulated)\n",
                "\n",
                "We'll use `Llama` class to load the GGUF model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from llama_cpp import Llama\n",
                "\n",
                "# Load model (set n_gpu_layers=0 to simulate pure CPU edge device)\n",
                "llm = Llama(\n",
                "    model_path=\"./smollm-135m-q4.gguf\",\n",
                "    n_ctx=2048,\n",
                "    n_gpu_layers=0,  # Run purely on CPU\n",
                "    verbose=False\n",
                ")\n",
                "\n",
                "print(\"ü§ñ Model Loaded on CPU\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4Ô∏è‚É£ Edge Inference Loop\n",
                "\n",
                "A simple chat loop optimized for low latency."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def chat_with_edge_model(prompt):\n",
                "    # Format prompt for SmolLM/ChatML\n",
                "    # <|im_start|>user\\n...<|im_end|>\\n<|im_start|>assistant\\n\n",
                "    full_prompt = f\"<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
                "    \n",
                "    stream = llm(\n",
                "        full_prompt,\n",
                "        max_tokens=100,\n",
                "        stop=[\"<|im_end|>\"],\n",
                "        echo=False,\n",
                "        stream=True  # Stream for perceived speed!\n",
                "    )\n",
                "    \n",
                "    print(\"EdgeAI: \", end=\"\", flush=True)\n",
                "    response = \"\"\n",
                "    for chunk in stream:\n",
                "        text = chunk['choices'][0]['text']\n",
                "        print(text, end=\"\", flush=True)\n",
                "        response += text\n",
                "    print(\"\\n\")\n",
                "    return response\n",
                "\n",
                "chat_with_edge_model(\"What is the best way to save energy?\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5Ô∏è‚É£ Building a Standalone Edge App\n",
                "\n",
                "Save this script as `app.py` and run it on your Pi!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "app_code = \"\"\"\n",
                "from llama_cpp import Llama\n",
                "import sys\n",
                "\n",
                "print(\"Loading model...\")\n",
                "llm = Llama(model_path=\"./smollm-135m-q4.gguf\", verbose=False)\n",
                "\n",
                "print(\"Ready! Type 'exit' to quit.\")\n",
                "while True:\n",
                "    user_input = input(\"User: \")\n",
                "    if user_input.lower() == \"exit\":\n",
                "        break\n",
                "    \n",
                "    prompt = f\"<|im_start|>user\\\\n{user_input}<|im_end|>\\\\n<|im_start|>assistant\\\\n\"\n",
                "    output = llm(prompt, max_tokens=128, stop=[\"<|im_end|>\"])\n",
                "    print(f\"AI: {output['choices'][0]['text']}\")\n",
                "\"\"\"\n",
                "\n",
                "with open(\"edge_app.py\", \"w\") as f:\n",
                "    f.write(app_code)\n",
                "\n",
                "print(\"‚úÖ Created edge_app.py\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üéâ Summary\n",
                "\n",
                "You've learned how to:\n",
                "- ‚úÖ Convert models for edge usage\n",
                "- ‚úÖ Run inference on CPU\n",
                "- ‚úÖ Create a standalone script for Raspberry Pi\n",
                "\n",
                "### Tips for Raspberry Pi\n",
                "- Use 64-bit OS (Raspberry Pi OS 64-bit).\n",
                "- Overclock slightly for 10-20% speedup.\n",
                "- Use a cooling fan!\n",
                "\n",
                "### Next Steps\n",
                "- Copy `edge_app.py` and `smollm-135m-q4.gguf` to your device and run!"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
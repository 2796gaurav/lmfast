{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üì¶ LMFast: Quantization & Export\n",
                "\n",
                "**Shrink your models for deployment without losing intelligence!**\n",
                "\n",
                "## What You'll Learn\n",
                "- 4-bit (QLoRA) vs 8-bit quantization\n",
                "- Export to GGUF (for llama.cpp / Ollama)\n",
                "- Export to ONNX (for standard runtimes)\n",
                "- Understand AWQ vs GPTQ\n",
                "\n",
                "## Quick Guide\n",
                "| Format | Best For | Speed | Size |\n",
                "|--------|----------|-------|------|\n",
                "| **GGUF** | CPU / Mac / Edge | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |\n",
                "| **Int4** | GPU Serving | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |\n",
                "| **ONNX** | Browser / Web | ‚≠ê‚≠ê | ‚≠ê‚≠ê |\n",
                "\n",
                "**Time to complete:** ~10 minutes"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1Ô∏è‚É£ Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q lmfast[all]\n",
                "\n",
                "import lmfast\n",
                "lmfast.setup_colab_env()\n",
                "\n",
                "import torch\n",
                "print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2Ô∏è‚É£ Load a Model\n",
                "\n",
                "We'll use a small model for demonstration."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Using the base model for export demos\n",
                "MODEL_ID = \"HuggingFaceTB/SmolLM-135M-Instruct\"\n",
                "\n",
                "# You can also point to your locally trained model:\n",
                "# MODEL_ID = \"./my_first_slm\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3Ô∏è‚É£ Export to GGUF (llama.cpp)\n",
                "\n",
                "GGUF is the gold standard for running LLMs on consumer hardware (MacBooks, Android, Raspberry Pi)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from lmfast.inference import export_gguf\n",
                "\n",
                "print(\"üì¶ Exporting to GGUF (q4_k_m)...\")\n",
                "print(\"Note: This requires 'llama.cpp' built or installed in environment.\")\n",
                "print(\"LMFast attempts to use the python bindings or system binary.\")\n",
                "\n",
                "try:\n",
                "    export_gguf(\n",
                "        model_path=MODEL_ID,\n",
                "        output_path=\"./smollm-135m-q4.gguf\",\n",
                "        quantization=\"q4_k_m\"  # Balanced 4-bit quantization\n",
                "    )\n",
                "    print(\"‚úÖ GGUF Export Successful!\")\n",
                "    \n",
                "    # Check size\n",
                "    import os\n",
                "    size_mb = os.path.getsize(\"./smollm-135m-q4.gguf\") / 1024 / 1024\n",
                "    print(f\"File Size: {size_mb:.2f} MB\")\n",
                "    \n",
                "except Exception as e:\n",
                "    print(f\"‚ö†Ô∏è GGUF Export skipped/failed: {e}\")\n",
                "    print(\"Run 'pip install llama-cpp-python' or install system tools.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4Ô∏è‚É£ In-Place Quantization (Int4 / Int8)\n",
                "\n",
                "If you want to serve the model using Python (transfomers/bitsandbytes), you can save a quantized version."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from lmfast.inference import quantize_model\n",
                "\n",
                "print(\"‚öñÔ∏è Quantizing to 4-bit (NF4)...\")\n",
                "\n",
                "quantize_model(\n",
                "    model_path=MODEL_ID,\n",
                "    output_dir=\"./smollm-int4\",\n",
                "    method=\"int4\"  # Uses bitsandbytes NF4\n",
                ")\n",
                "\n",
                "print(\"‚úÖ Int4 Model Saved!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5Ô∏è‚É£ Export to ONNX\n",
                "\n",
                "Great for running in wider ecosystems (C#, Java, Web)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from lmfast.deployment import export_for_browser\n",
                "\n",
                "# The browser exporter handles ONNX conversion internally\n",
                "artifacts = export_for_browser(\n",
                "    model_path=MODEL_ID,\n",
                "    output_dir=\"./onnx_model\",\n",
                "    target=\"onnx\",\n",
                "    quantization=\"int8\", # Quantize weights for size\n",
                "    create_demo=False\n",
                ")\n",
                "\n",
                "print(f\"‚úÖ ONNX paths: {artifacts}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6Ô∏è‚É£ Verify the Quantized Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from lmfast.inference import SLMServer\n",
                "\n",
                "# Load the int4 model we saved earlier\n",
                "server = SLMServer(\"./smollm-int4\")\n",
                "\n",
                "response = server.generate(\"What is the speed of light?\")\n",
                "print(f\"Output from Int4 model: {response}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üéâ Summary\n",
                "\n",
                "You've learned how to:\n",
                "- ‚úÖ Create GGUF files for edge deployment\n",
                "- ‚úÖ Save 4-bit/8-bit models for Python serving\n",
                "- ‚úÖ Export ONNX models for interoperability\n",
                "\n",
                "### Next Steps\n",
                "- `15_browser_deployment.ipynb`: Use the ONNX model in a web app!\n",
                "- `16_edge_deployment.ipynb`: Run the GGUF model on a Raspberry Pi"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}